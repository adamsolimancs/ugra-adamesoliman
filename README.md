# Tracking Famous People's Residences from Wikipedia Dumps
## Overview

Offline pipeline that parses Wikipedia dumps, finds sentences about where notable people lived, and normalizes them into structured JSONL via an LLM with regex-based quality control.

---
### Setup
- Python 3 and `pip`
- Install deps: `pip install -r requirements.txt`
- Data layout:
  - `wiki_dumps/` containing one or more `*.xml.bz2` Wikipedia dumps.
  - `notable_humans/result.csv` listing the people to keep.
  - OPTIONAL: `residence_classifier.joblib` (generated by `train_classifier.py` if needed) for the sentence classifier.

Environment for the LLM step:
- `OLLAMA_HOST` (default `http://127.0.0.1:11434`)
- `LLM_PRIMARY_MODEL` (default `llama3.1:8b`)

---
### Pipeline components
- `parser.py` — multiprocessing parse of dumps into `wiki_articles.jsonl` (one JSON object per article).
- `extractor.py` — loads the SentenceTransformer + logistic regression model, scores sentences, and writes `{"name": ..., "residence_sentences": [...]}` to a JSONL you choose.
- `llm_processing.py` — calls the local LLM to turn residence sentences into structured records with regex guards for place/time/evidence, emitting `structured_residences.jsonl`.
- `train_classifier.py` — fits the sentence classifier from `train_data.jsonl` and saves `residence_classifier.joblib`.
- `main.py` — orchestration script that parses dumps, extracts sentences, and then calls the LLM step (adjust paths as needed).

---
### Running the pipeline (manual steps without main.py)
1) Parse dumps to articles:
```bash
python - <<'PY'
from parser import parse_wiki_dump
parse_wiki_dump("wiki_dumps/your_dump.xml.bz2", "wiki_articles.jsonl")
PY
```

2) Extract residence sentences for notable people:
```bash
python - <<'PY'
from extractor import process_pages
process_pages("wiki_articles.jsonl", "llm_output_residences.jsonl", "notable_humans/result.csv")
PY
```

3) Normalize with the LLM (regex QC included):
```bash
python llm_processing.py  # reads llm_output_residences.jsonl, writes structured_residences.jsonl
```

Optional: batch-test the LLM with synthetic prompts:
```bash
python tests/test_llm_batch.py
```
Optional flags: `--count 100 --concurrency 5 --input llm_output_residences.jsonl`

Time a full LLM run (reads llm_output_residences.jsonl by default):
```bash
python tests/time_llm_processing.py --output structured_residences_2.jsonl
```

Tip: `main.py` wires the steps together; align its file names with the inputs/outputs above if you customize paths.

---
### Notes on quality control
- `llm_processing.py` extracts the first JSON block from each LLM reply and drops entries with obviously invalid `place` fields.
- `time_span` and `evidence` are blanked if they do not match simple regex cues (years, ranges, eras, or text content).
